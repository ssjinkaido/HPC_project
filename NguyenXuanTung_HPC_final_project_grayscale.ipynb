{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student name: Nguyen Xuan Tung\n",
    "### Student ID: M22.ICT.006\n",
    "### Version1 : Apply Histogram equalization on grayscale image\n",
    "### It is recommended to restart the notebook after running each cell to get consistent results and avoid out-of-memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average kernel computation time is 0.31411840319633483 ms\n",
      "[570.285400390625, 0.44521600008010864, 0.2789120078086853, 0.2861120104789734, 0.28195199370384216, 0.2784000039100647]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class HistogramEqualization(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "    _cache_4 = {}\n",
    "    _cache_5 = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_3d_to_2d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx][0] = d_input_orig[y, x][0]\n",
    "                d_input[idx][1] = d_input_orig[y, x][1]\n",
    "                d_input[idx][2] = d_input_orig[y, x][2]\n",
    "\n",
    "        return cuda.jit(reshape_3d_to_2d)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = HistogramEqualization._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = (\n",
    "            HistogramEqualization._NUM_WARPS * HistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_input\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_r[local_tid] = d_input[global_tid][0]\n",
    "            shared_memory_g[local_tid] = d_input[global_tid][1]\n",
    "            shared_memory_b[local_tid] = d_input[global_tid][2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_cdf(histogram, cdf):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            cdf[local_tid] = histogram[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_output\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            grayscale_value = round(\n",
    "                shared_memory_r[local_tid] * 0.299\n",
    "                + shared_memory_g[local_tid] * 0.587\n",
    "                + shared_memory_b[local_tid] * 0.114\n",
    "            )\n",
    "            d_output[global_tid] = grayscale_value\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory_histogram(shared_memory_histogram, d_hist):\n",
    "            tid = cuda.threadIdx.x\n",
    "            cuda.syncthreads()\n",
    "            while tid < max_block_size:\n",
    "                cuda.atomic.add(d_hist, tid, shared_memory_histogram[tid])\n",
    "                tid += cuda.blockDim.x\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory_histogram(shared_memory_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            shared_memory_histogram[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def histogram(d_input, d_output, d_hist, d_cdf):\n",
    "            shared_memory_red = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_green = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_blue = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "\n",
    "            load_shared_memory_input(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_input\n",
    "            )\n",
    "\n",
    "            grayscale_image(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_output\n",
    "            )\n",
    "\n",
    "            shared_memory_histogram = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            init_shared_memory_histogram(shared_memory_histogram)\n",
    "            tid = cuda.grid(1)\n",
    "            gridDim = cuda.gridDim.x * cuda.blockDim.x\n",
    "            while tid < d_output.size:\n",
    "                pixel_values = d_output[tid]\n",
    "                cuda.atomic.add(shared_memory_histogram, pixel_values, 1)\n",
    "                tid += gridDim\n",
    "            save_shared_memory_histogram(shared_memory_histogram, d_hist)\n",
    "\n",
    "            load_shared_memory_cdf(d_hist, d_cdf)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = HistogramEqualization._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        max_block_size = (\n",
    "            HistogramEqualization._NUM_WARPS * HistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf(cdf) -> None:\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(cdf, jump)\n",
    "                jump *= 2\n",
    "            cuda.syncthreads()\n",
    "            max_value = cdf[-1]\n",
    "            if max_value > 0 and local_tid < max_block_size:\n",
    "                temp = cdf[local_tid]\n",
    "                cdf[local_tid] = (temp * 255) // max_value\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def cdf(d_cdf):\n",
    "            calculate_cdf(d_cdf)\n",
    "\n",
    "        return cuda.jit(cdf)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = HistogramEqualization._gpu_kernel_factory_3(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_4():\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_histogram_equalization(cdf, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            temp = d_output[global_tid]\n",
    "            d_output[global_tid] = cdf[temp]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def histogram_equalization(cdf, d_output):\n",
    "            calculate_histogram_equalization(cdf, d_output)\n",
    "\n",
    "        return cuda.jit(histogram_equalization)\n",
    "\n",
    "    def _compile_4(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_4:\n",
    "            self._cache_4[key] = HistogramEqualization._gpu_kernel_factory_4()\n",
    "        return self._cache_4[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_5():\n",
    "        def reshape_1d_to_2d(d_input, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            width = d_output.shape[1]\n",
    "            if global_tid < d_input.size:\n",
    "                row = global_tid // width\n",
    "                column = global_tid % width\n",
    "\n",
    "                d_output[row, column] = d_input[global_tid]\n",
    "\n",
    "        return cuda.jit(reshape_1d_to_2d)\n",
    "\n",
    "    def _compile_5(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_5:\n",
    "            self._cache_5[key] = HistogramEqualization._gpu_kernel_factory_5()\n",
    "        return self._cache_5[key]\n",
    "\n",
    "    def __call__(self, d_input_orig):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        max_block_size = (\n",
    "            HistogramEqualization._WARP_SIZE * HistogramEqualization._NUM_WARPS\n",
    "        )\n",
    "        stream = cuda.default_stream()\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(d_input_orig.dtype)\n",
    "        kernel4 = self._compile_4(d_input_orig.dtype)\n",
    "        kernel5 = self._compile_5(d_input_orig.dtype)\n",
    "\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width, 3), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_final_output_1d = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        d_final_output_2d = cuda.device_array(\n",
    "            shape=(height, width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        d_hist = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        d_cdf = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "        nb_threads = max_block_size\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = min(max_block_size, d_input.size)\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "\n",
    "        kernel2[nb_blocks, nb_threads, stream](\n",
    "            d_input, d_final_output_1d, d_hist, d_cdf\n",
    "        )\n",
    "        kernel3[1, nb_threads, stream](d_cdf)\n",
    "        kernel4[nb_blocks, nb_threads, stream](d_cdf, d_final_output_1d)\n",
    "        kernel5[nb_blocks, nb_threads, stream](d_final_output_1d, d_final_output_2d)\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return (\n",
    "            cuda.event_elapsed_time(start_event, stop_event),\n",
    "            d_final_output_2d.copy_to_host(),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def histogram_equalization():\n",
    "        he = HistogramEqualization(lambda a, b: a + b)\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image = Image.open(image_bytes)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        d_input_image = cuda.to_device(image)\n",
    "        ct = []\n",
    "        for i in range(6):\n",
    "            if i == 0:\n",
    "                time, result_image = he(d_input_image)\n",
    "                result_image = Image.fromarray(result_image)\n",
    "                result_image.save(\"he.png\")\n",
    "            else:\n",
    "                time, _ = he(d_input_image)\n",
    "            ct.append(time)\n",
    "        print(f\"average kernel computation time is {np.average(ct[1:])} ms\")\n",
    "        print(ct)\n",
    "\n",
    "    histogram_equalization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average kernel computation time is 0.3777919948101044 ms\n",
      "[568.291748046875, 0.5232959985733032, 0.34463998675346375, 0.3468799889087677, 0.33478400111198425, 0.33935999870300293]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class AdjustedHistogramEqualization(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "    _cache_4 = {}\n",
    "    _cache_5 = {}\n",
    "    _cache_6 = {}\n",
    "    _cache_7 = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_3d_to_2d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx][0] = d_input_orig[y, x][0]\n",
    "                d_input[idx][1] = d_input_orig[y, x][1]\n",
    "                d_input[idx][2] = d_input_orig[y, x][2]\n",
    "\n",
    "        return cuda.jit(reshape_3d_to_2d)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = AdjustedHistogramEqualization._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = (\n",
    "            AdjustedHistogramEqualization._NUM_WARPS\n",
    "            * AdjustedHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_input\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_r[local_tid] = d_input[global_tid][0]\n",
    "            shared_memory_g[local_tid] = d_input[global_tid][1]\n",
    "            shared_memory_b[local_tid] = d_input[global_tid][2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_output\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            grayscale_value = round(\n",
    "                shared_memory_r[local_tid] * 0.299\n",
    "                + shared_memory_g[local_tid] * 0.587\n",
    "                + shared_memory_b[local_tid] * 0.114\n",
    "            )\n",
    "            d_output[global_tid] = grayscale_value\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory_histogram(shared_memory_histogram, d_hist):\n",
    "            tid = cuda.threadIdx.x\n",
    "            cuda.syncthreads()\n",
    "            while tid < max_block_size:\n",
    "                cuda.atomic.add(d_hist, tid, shared_memory_histogram[tid])\n",
    "                tid += cuda.blockDim.x\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory_histogram(shared_memory_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            shared_memory_histogram[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "            \n",
    "\n",
    "        def histogram(d_input, d_output, d_hist):\n",
    "            shared_memory_red = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_green = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_blue = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "\n",
    "            load_shared_memory_input(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_input\n",
    "            )\n",
    "\n",
    "            grayscale_image(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_output\n",
    "            )\n",
    "\n",
    "            shared_memory_histogram = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            init_shared_memory_histogram(shared_memory_histogram)\n",
    "            tid = cuda.grid(1)\n",
    "            gridDim = cuda.gridDim.x * cuda.blockDim.x\n",
    "            while tid < d_output.size:\n",
    "                pixel_values = d_output[tid]\n",
    "                cuda.atomic.add(shared_memory_histogram, pixel_values, 1)\n",
    "                tid += gridDim\n",
    "            save_shared_memory_histogram(shared_memory_histogram, d_hist)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = AdjustedHistogramEqualization._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3():\n",
    "        @cuda.jit(device=True)\n",
    "        def create_uniform_histogram(d_uniform_hist, number_of_pixels):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "\n",
    "            pixels_per_bin = number_of_pixels // 256\n",
    "\n",
    "            remainder = number_of_pixels - pixels_per_bin * 256\n",
    "\n",
    "            d_uniform_hist[local_tid] = pixels_per_bin\n",
    "\n",
    "            if local_tid < remainder:\n",
    "                d_uniform_hist[local_tid] += 1\n",
    "\n",
    "        def uniform_histogram(d_uniform_hist, number_of_pixels):\n",
    "            create_uniform_histogram(d_uniform_hist, number_of_pixels)\n",
    "\n",
    "        return cuda.jit(uniform_histogram)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = AdjustedHistogramEqualization._gpu_kernel_factory_3()\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_4():\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_adjusted_histogram(\n",
    "            d_uniform_hist, d_hist, d_adjusted_hist, lambda_value\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            d_adjusted_hist[local_tid] = (1 / (1 + lambda_value)) * d_hist[\n",
    "                local_tid\n",
    "            ] + (lambda_value / (1 + lambda_value)) * d_uniform_hist[local_tid]\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_cdf(histogram, cdf):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            cdf[local_tid] = histogram[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def adjusted_histogram(\n",
    "            d_uniform_hist, d_hist, d_adjusted_hist, lambda_value, d_cdf\n",
    "        ):\n",
    "            calculate_adjusted_histogram(\n",
    "                d_uniform_hist, d_hist, d_adjusted_hist, lambda_value\n",
    "            )\n",
    "            load_shared_memory_cdf(d_adjusted_hist, d_cdf)\n",
    "\n",
    "        return cuda.jit(adjusted_histogram)\n",
    "\n",
    "    def _compile_4(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_4:\n",
    "            self._cache_4[key] = AdjustedHistogramEqualization._gpu_kernel_factory_4()\n",
    "        return self._cache_4[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_5(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        max_block_size = (\n",
    "            AdjustedHistogramEqualization._NUM_WARPS\n",
    "            * AdjustedHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf(cdf) -> None:\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(cdf, jump)\n",
    "                jump *= 2\n",
    "            cuda.syncthreads()\n",
    "            max_value = cdf[-1]\n",
    "            if max_value > 0 and local_tid < max_block_size:\n",
    "                temp = cdf[local_tid]\n",
    "                cdf[local_tid] = (temp * 255) // max_value\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def cdf(d_cdf):\n",
    "            calculate_cdf(d_cdf)\n",
    "\n",
    "        return cuda.jit(cdf)\n",
    "\n",
    "    def _compile_5(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_5:\n",
    "            self._cache_5[key] = AdjustedHistogramEqualization._gpu_kernel_factory_5(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_5[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_6():\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_histogram_equalization(cdf, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            temp = d_output[global_tid]\n",
    "            d_output[global_tid] = cdf[temp]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def histogram_equalization(cdf, d_output):\n",
    "            calculate_histogram_equalization(cdf, d_output)\n",
    "\n",
    "        return cuda.jit(histogram_equalization)\n",
    "\n",
    "    def _compile_6(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_6:\n",
    "            self._cache_6[key] = AdjustedHistogramEqualization._gpu_kernel_factory_6()\n",
    "        return self._cache_6[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_7():\n",
    "        def reshape_1d_to_2d(d_input, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            width = d_output.shape[1]\n",
    "            if global_tid < d_input.size:\n",
    "                row = global_tid // width\n",
    "                column = global_tid % width\n",
    "\n",
    "                d_output[row, column] = d_input[global_tid]\n",
    "\n",
    "        return cuda.jit(reshape_1d_to_2d)\n",
    "\n",
    "    def _compile_7(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_7:\n",
    "            self._cache_7[key] = AdjustedHistogramEqualization._gpu_kernel_factory_7()\n",
    "        return self._cache_7[key]\n",
    "\n",
    "    def __call__(self, d_input_orig, lambda_value):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        max_block_size = (\n",
    "            AdjustedHistogramEqualization._WARP_SIZE\n",
    "            * AdjustedHistogramEqualization._NUM_WARPS\n",
    "        )\n",
    "        stream = cuda.default_stream()\n",
    "\n",
    "        nb_threads = min(max_block_size, d_input_orig.size)\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "        number_of_pixels = height * width\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width, 3), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_hist = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        d_cdf = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        d_uniform_hist = cuda.device_array(\n",
    "            shape=(256,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_adjusted_hist = cuda.device_array(\n",
    "            shape=(256,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_final_output_1d = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        d_final_output_2d = cuda.device_array(\n",
    "            shape=(height, width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(\n",
    "            d_input_orig.dtype,\n",
    "        )\n",
    "        kernel4 = self._compile_4(d_input_orig.dtype)\n",
    "        kernel5 = self._compile_5(d_input_orig.dtype)\n",
    "        kernel6 = self._compile_6(d_input_orig.dtype)\n",
    "        kernel7 = self._compile_7(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = min(max_block_size, d_input_orig.size)\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "\n",
    "        kernel2[nb_blocks, nb_threads, stream](d_input, d_final_output_1d, d_hist)\n",
    "        kernel3[1, nb_threads, stream](d_uniform_hist, number_of_pixels)\n",
    "        kernel4[1, nb_threads, stream](\n",
    "            d_uniform_hist, d_hist, d_adjusted_hist, lambda_value, d_cdf\n",
    "        )\n",
    "        kernel5[1, nb_threads, stream](d_cdf)\n",
    "        kernel6[nb_blocks, nb_threads, stream](d_cdf, d_final_output_1d)\n",
    "        kernel7[nb_blocks, nb_threads, stream](d_final_output_1d, d_final_output_2d)\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return (\n",
    "            cuda.event_elapsed_time(start_event, stop_event),\n",
    "            d_final_output_2d.copy_to_host(),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def adjusted_histogram_equalization():\n",
    "        ahe = AdjustedHistogramEqualization(lambda a, b: a + b)\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image = Image.open(image_bytes)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "\n",
    "        d_input_image = cuda.to_device(image)\n",
    "        lambda_value = 3\n",
    "        ct = []\n",
    "        for i in range(6):\n",
    "            if i == 0:\n",
    "                time, result_image = ahe(d_input_image, lambda_value)\n",
    "                result_image = Image.fromarray(result_image)\n",
    "                result_image.save(\"ahe.png\")\n",
    "            else:\n",
    "                time, _ = ahe(d_input_image, lambda_value)\n",
    "            ct.append(time)\n",
    "        print(f\"average kernel computation time is {np.average(ct[1:])} ms\")\n",
    "        print(ct)\n",
    "\n",
    "    adjusted_histogram_equalization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average kernel computation time is 1.024889588356018 ms\n",
      "[759.64306640625, 1.1565120220184326, 0.9907199740409851, 0.989471971988678, 0.9939519762992859, 0.9937919974327087]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class WeightedHistogramEqualization(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "    _cache_4 = {}\n",
    "    _cache_5 = {}\n",
    "    _cache_6 = {}\n",
    "    _cache_7 = {}\n",
    "    _cache_8 = {}\n",
    "    _cache_9 = {}\n",
    "    _cache_10 = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_3d_to_2d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx][0] = d_input_orig[y, x][0]\n",
    "                d_input[idx][1] = d_input_orig[y, x][1]\n",
    "                d_input[idx][2] = d_input_orig[y, x][2]\n",
    "\n",
    "        return cuda.jit(reshape_3d_to_2d)\n",
    "\n",
    "        return cuda.jit(reshape)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = WeightedHistogramEqualization._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = (\n",
    "            WeightedHistogramEqualization._NUM_WARPS\n",
    "            * WeightedHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_input\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_r[local_tid] = d_input[global_tid][0]\n",
    "            shared_memory_g[local_tid] = d_input[global_tid][1]\n",
    "            shared_memory_b[local_tid] = d_input[global_tid][2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_output\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            grayscale_value = round(\n",
    "                shared_memory_r[local_tid] * 0.299\n",
    "                + shared_memory_g[local_tid] * 0.587\n",
    "                + shared_memory_b[local_tid] * 0.114\n",
    "            )\n",
    "            d_output[global_tid] = grayscale_value\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory_histogram(shared_memory_histogram, d_hist):\n",
    "            tid = cuda.threadIdx.x\n",
    "            cuda.syncthreads()\n",
    "            while tid < max_block_size:\n",
    "                cuda.atomic.add(d_hist, tid, shared_memory_histogram[tid])\n",
    "                tid += cuda.blockDim.x\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory_histogram(shared_memory_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            shared_memory_histogram[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def histogram(d_input, d_output, d_hist):\n",
    "            shared_memory_red = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_green = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_blue = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "\n",
    "            load_shared_memory_input(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_input\n",
    "            )\n",
    "\n",
    "            grayscale_image(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_output\n",
    "            )\n",
    "\n",
    "            shared_memory_histogram = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            init_shared_memory_histogram(shared_memory_histogram)\n",
    "            tid = cuda.grid(1)\n",
    "            gridDim = cuda.gridDim.x * cuda.blockDim.x\n",
    "            while tid < d_output.size:\n",
    "                pixel_values = d_output[tid]\n",
    "                cuda.atomic.add(shared_memory_histogram, pixel_values, 1)\n",
    "                tid += gridDim\n",
    "            save_shared_memory_histogram(shared_memory_histogram, d_hist)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = WeightedHistogramEqualization._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3():\n",
    "        @cuda.jit\n",
    "        def create_uniform_histogram(d_uniform_hist, number_of_pixels):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "\n",
    "            pixels_per_bin = number_of_pixels // 256\n",
    "\n",
    "            remainder = number_of_pixels - pixels_per_bin * 256\n",
    "\n",
    "            d_uniform_hist[local_tid] = pixels_per_bin\n",
    "\n",
    "            if local_tid < remainder:\n",
    "                d_uniform_hist[local_tid] += 1\n",
    "\n",
    "        def uniform_histogram(d_uniform_hist, number_of_pixels):\n",
    "            create_uniform_histogram(d_uniform_hist, number_of_pixels)\n",
    "\n",
    "        return cuda.jit(uniform_histogram)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = WeightedHistogramEqualization._gpu_kernel_factory_3()\n",
    "\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_4():\n",
    "        def calculate_local_variance(d_output, d_local_var, height, width, window_size):\n",
    "            x, y = cuda.grid(2)\n",
    "            radius = window_size // 2\n",
    "\n",
    "            sum_pixels = 0.0\n",
    "            sum_pixels_squared = 0.0\n",
    "            num_pixels = 0\n",
    "            for dy in range(-radius, radius + 1):\n",
    "                for dx in range(-radius, radius + 1):\n",
    "                    new_x = x + dx\n",
    "                    new_y = y + dy\n",
    "                    if new_x >= 0 and new_x < width and new_y >= 0 and new_y < height:\n",
    "                        pixel_value = d_output[new_y * width + new_x]\n",
    "                        sum_pixels += pixel_value\n",
    "                        sum_pixels_squared += pixel_value**2\n",
    "                        num_pixels += 1\n",
    "                    else:\n",
    "                        continue\n",
    "            mean = sum_pixels / num_pixels\n",
    "            variance = (sum_pixels_squared / num_pixels) - (mean**2)\n",
    "            d_local_var[y * width + x] = variance\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        return cuda.jit(calculate_local_variance)\n",
    "\n",
    "    def _compile_4(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_4:\n",
    "            self._cache_4[key] = WeightedHistogramEqualization._gpu_kernel_factory_4()\n",
    "\n",
    "        return self._cache_4[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_5():\n",
    "        def track_local_variance(d_output, d_local_var, d_total_var):\n",
    "            tid = cuda.grid(1)\n",
    "            if tid < d_output.size:\n",
    "                var = d_local_var[tid]\n",
    "                pixel_intensity = d_output[tid]\n",
    "                cuda.atomic.add(d_total_var, pixel_intensity, var)\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        return cuda.jit(track_local_variance)\n",
    "\n",
    "    def _compile_5(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_5:\n",
    "            self._cache_5[key] = WeightedHistogramEqualization._gpu_kernel_factory_5()\n",
    "\n",
    "        return self._cache_5[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_6():\n",
    "        def average_variance(d_average_local_var, d_total_var, d_hist):\n",
    "            tid = cuda.grid(1)\n",
    "            if d_total_var[tid] == 0:\n",
    "                d_average_local_var[tid] = 0\n",
    "            else:\n",
    "                d_average_local_var[tid] = d_total_var[tid] / d_hist[tid]\n",
    "\n",
    "        return cuda.jit(average_variance)\n",
    "\n",
    "    def _compile_6(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_6:\n",
    "            self._cache_6[key] = WeightedHistogramEqualization._gpu_kernel_factory_6()\n",
    "\n",
    "        return self._cache_6[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_7():\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_modified_hist(\n",
    "            d_average_local_var, d_hist, d_uniform_hist, d_modified_hist, lambda_value\n",
    "        ):\n",
    "            tid = cuda.grid(1)\n",
    "            d_modified_hist[tid] = (\n",
    "                d_average_local_var[tid] * d_hist[tid]\n",
    "                + lambda_value * d_uniform_hist[tid]\n",
    "            ) / (d_average_local_var[tid] + lambda_value)\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def modified_hist(\n",
    "            d_average_local_var, d_hist, d_uniform_hist, d_modified_hist, lambda_value\n",
    "        ):\n",
    "            calculate_modified_hist(\n",
    "                d_average_local_var,\n",
    "                d_hist,\n",
    "                d_uniform_hist,\n",
    "                d_modified_hist,\n",
    "                lambda_value,\n",
    "            )\n",
    "\n",
    "        return cuda.jit(modified_hist)\n",
    "\n",
    "    def _compile_7(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_7:\n",
    "            self._cache_7[key] = WeightedHistogramEqualization._gpu_kernel_factory_7()\n",
    "\n",
    "        return self._cache_7[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_8(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        max_block_size = (\n",
    "            WeightedHistogramEqualization._NUM_WARPS\n",
    "            * WeightedHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf(cdf) -> None:\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(cdf, jump)\n",
    "                jump *= 2\n",
    "            cuda.syncthreads()\n",
    "            max_value = cdf[-1]\n",
    "            if max_value > 0 and local_tid < max_block_size:\n",
    "                temp = cdf[local_tid]\n",
    "                cdf[local_tid] = (temp * 255) // max_value\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def cdf(d_cdf):\n",
    "            calculate_cdf(d_cdf)\n",
    "\n",
    "        return cuda.jit(cdf)\n",
    "\n",
    "    def _compile_8(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_8:\n",
    "            self._cache_8[key] = WeightedHistogramEqualization._gpu_kernel_factory_8(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_8[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_9():\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_weighted_histogram_equalization(cdf, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            temp = d_output[global_tid]\n",
    "            d_output[global_tid] = cdf[temp]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def weighted_histogram_equalization(cdf, d_output):\n",
    "            calculate_weighted_histogram_equalization(cdf, d_output)\n",
    "\n",
    "        return cuda.jit(weighted_histogram_equalization)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_10():\n",
    "        def reshape_1d_to_2d(d_input, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            width = d_output.shape[1]\n",
    "            if global_tid < d_input.size:\n",
    "                row = global_tid // width\n",
    "                column = global_tid % width\n",
    "\n",
    "                d_output[row, column] = d_input[global_tid]\n",
    "\n",
    "        return cuda.jit(reshape_1d_to_2d)\n",
    "\n",
    "    def _compile_10(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_10:\n",
    "            self._cache_10[key] = WeightedHistogramEqualization._gpu_kernel_factory_10()\n",
    "        return self._cache_10[key]\n",
    "\n",
    "    def _compile_9(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_9:\n",
    "            self._cache_9[key] = WeightedHistogramEqualization._gpu_kernel_factory_9()\n",
    "        return self._cache_9[key]\n",
    "\n",
    "    def __call__(self, d_input_orig, lambda_value, window_size):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        max_block_size = (\n",
    "            WeightedHistogramEqualization._WARP_SIZE\n",
    "            * WeightedHistogramEqualization._NUM_WARPS\n",
    "        )\n",
    "        stream = cuda.default_stream()\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width, 3), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_final_output_1d = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        d_final_output_2d = cuda.device_array(\n",
    "            shape=(height, width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        number_of_pixels = d_input.size\n",
    "        d_hist = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        d_uniform_hist = cuda.device_array(\n",
    "            shape=(256,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_modified_hist = cuda.device_array(\n",
    "            shape=(256,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_local_var = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_total_var = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        d_average_local_var = cuda.device_array(\n",
    "            shape=(256,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(\n",
    "            d_input_orig.dtype,\n",
    "        )\n",
    "        kernel4 = self._compile_4(d_input_orig.dtype)\n",
    "        kernel5 = self._compile_5(d_input_orig.dtype)\n",
    "        kernel6 = self._compile_6(d_input_orig.dtype)\n",
    "        kernel7 = self._compile_7(d_input_orig.dtype)\n",
    "        kernel8 = self._compile_8(d_input_orig.dtype)\n",
    "        kernel9 = self._compile_9(d_input_orig.dtype)\n",
    "        kernel10 = self._compile_10(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = min(max_block_size, d_input_orig.size)\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "        kernel2[nb_blocks, nb_threads, stream](d_input, d_final_output_1d, d_hist)\n",
    "        kernel3[1, nb_threads, stream](d_uniform_hist, number_of_pixels)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel4[nb_blocks, nb_threads](\n",
    "            d_final_output_1d, d_local_var, height, width, window_size\n",
    "        )\n",
    "        nb_threads = min(max_block_size, d_input_orig.size)\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "        kernel5[nb_blocks, nb_threads, stream](\n",
    "            d_final_output_1d, d_local_var, d_total_var\n",
    "        )\n",
    "        kernel6[1, nb_threads, stream](d_average_local_var, d_total_var, d_hist)\n",
    "        kernel7[1, nb_threads, stream](\n",
    "            d_average_local_var, d_hist, d_uniform_hist, d_modified_hist, lambda_value\n",
    "        )\n",
    "        kernel8[1, nb_threads, stream](d_modified_hist)\n",
    "        kernel9[nb_blocks, nb_threads, stream](d_modified_hist, d_final_output_1d)\n",
    "        kernel10[nb_blocks, nb_threads, stream](d_final_output_1d, d_final_output_2d)\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return (\n",
    "            cuda.event_elapsed_time(start_event, stop_event),\n",
    "            d_final_output_2d.copy_to_host(),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def weighted_histogram_equalization():\n",
    "        whe = WeightedHistogramEqualization(lambda a, b: a + b)\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image = Image.open(image_bytes)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        d_input_image = cuda.to_device(image)\n",
    "        lambda_value = 3\n",
    "        window_size = 3\n",
    "        ct = []\n",
    "        for i in range(6):\n",
    "            if i == 0:\n",
    "                time, result_image = whe(\n",
    "                    d_input_image,\n",
    "                    lambda_value,\n",
    "                    window_size,\n",
    "                )\n",
    "                result_image = Image.fromarray(result_image)\n",
    "                result_image.save(\"whe.png\")\n",
    "            else:\n",
    "                time, _ = whe(\n",
    "                    d_input_image,\n",
    "                    lambda_value,\n",
    "                    window_size,\n",
    "                )\n",
    "            ct.append(time)\n",
    "        print(f\"average kernel computation time is {np.average(ct[1:])} ms\")\n",
    "        print(ct)\n",
    "\n",
    "    weighted_histogram_equalization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESIHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average kernel computation time is 0.7698495984077454 ms\n",
      "[1134.9710693359375, 1.0113919973373413, 0.7313600182533264, 0.707423985004425, 0.7080000042915344, 0.6910719871520996]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class ExposureHistogramEqualization(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "    _cache_4 = {}\n",
    "    _cache_5 = {}\n",
    "    _cache_6 = {}\n",
    "    _cache_7 = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_3d_to_2d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx][0] = d_input_orig[y, x][0]\n",
    "                d_input[idx][1] = d_input_orig[y, x][1]\n",
    "                d_input[idx][2] = d_input_orig[y, x][2]\n",
    "\n",
    "        return cuda.jit(reshape_3d_to_2d)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = ExposureHistogramEqualization._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = (\n",
    "            ExposureHistogramEqualization._NUM_WARPS\n",
    "            * ExposureHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_input\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_r[local_tid] = d_input[global_tid][0]\n",
    "            shared_memory_g[local_tid] = d_input[global_tid][1]\n",
    "            shared_memory_b[local_tid] = d_input[global_tid][2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_output\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            grayscale_value = round(\n",
    "                shared_memory_r[local_tid] * 0.299\n",
    "                + shared_memory_g[local_tid] * 0.587\n",
    "                + shared_memory_b[local_tid] * 0.114\n",
    "            )\n",
    "            d_output[global_tid] = grayscale_value\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory_histogram(shared_memory_histogram, d_hist):\n",
    "            tid = cuda.threadIdx.x\n",
    "            cuda.syncthreads()\n",
    "            while tid < max_block_size:\n",
    "                cuda.atomic.add(d_hist, tid, shared_memory_histogram[tid])\n",
    "                tid += cuda.blockDim.x\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory_histogram(shared_memory_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            shared_memory_histogram[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def histogram(d_input, d_output, d_hist):\n",
    "            shared_memory_red = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_green = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_blue = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "\n",
    "            load_shared_memory_input(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_input\n",
    "            )\n",
    "\n",
    "            grayscale_image(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_output\n",
    "            )\n",
    "\n",
    "            shared_memory_histogram = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            init_shared_memory_histogram(shared_memory_histogram)\n",
    "            tid = cuda.grid(1)\n",
    "            gridDim = cuda.gridDim.x * cuda.blockDim.x\n",
    "            while tid < d_output.size:\n",
    "                pixel_values = d_output[tid]\n",
    "                cuda.atomic.add(shared_memory_histogram, pixel_values, 1)\n",
    "                tid += gridDim\n",
    "            save_shared_memory_histogram(shared_memory_histogram, d_hist)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = ExposureHistogramEqualization._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "        max_block_size = (\n",
    "            ExposureHistogramEqualization._NUM_WARPS\n",
    "            * ExposureHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < arr.size:\n",
    "                shared_memory[local_tid] = arr[local_tid]\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_mean_histogram(d_hist, partials, mean_histogram, sum_histogram):\n",
    "            load_shared_memory(partials, d_hist)\n",
    "\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(partials, jump)\n",
    "                jump = jump * 2\n",
    "\n",
    "            if cuda.threadIdx.x == 0:\n",
    "                sum_histogram[0] = partials[255]\n",
    "                mean_histogram[0] = round(partials[255] / 256)\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def clip_histogram(d_hist, d_clip_hist, clip_value):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < max_block_size:\n",
    "                temp = d_hist[local_tid]\n",
    "                if temp > clip_value:\n",
    "                    d_clip_hist[local_tid] = clip_value\n",
    "                else:\n",
    "                    d_clip_hist[local_tid] = temp\n",
    "\n",
    "        def create_clip_histogram(d_hist, d_clip_hist, mean_histogram, sum_histogram):\n",
    "            partials = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "            calculate_mean_histogram(d_hist, partials, mean_histogram, sum_histogram)\n",
    "            clip_histogram(d_hist, d_clip_hist, mean_histogram[0])\n",
    "\n",
    "        return cuda.jit(create_clip_histogram)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = ExposureHistogramEqualization._gpu_kernel_factory_3(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_4(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "        max_block_size = (\n",
    "            ExposureHistogramEqualization._NUM_WARPS\n",
    "            * ExposureHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < arr.size:\n",
    "                shared_memory[local_tid] = arr[local_tid]\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_different_values(\n",
    "            partials, sum_histogram, exposure, a_norm, x_mean\n",
    "        ):\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(partials, jump)\n",
    "                jump = jump * 2\n",
    "\n",
    "            if cuda.threadIdx.x == 0:\n",
    "                exposure_value = (partials[255] / sum_histogram) / 256\n",
    "                a_norm_value = 1 - exposure_value\n",
    "                exposure[0] = exposure_value\n",
    "                a_norm[0] = a_norm_value\n",
    "                x_mean[0] = round(256 * a_norm_value)\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def multiply_index(\n",
    "            partials,\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < cuda.blockDim.x:\n",
    "                temp = partials[local_tid]\n",
    "                partials[local_tid] = temp * local_tid\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def exposure(d_hist, sum_histogram, exposure, a_norm, x_mean):\n",
    "            partials = cuda.shared.array(shape=256, dtype=np.int32)\n",
    "            load_shared_memory(partials, d_hist)\n",
    "\n",
    "            multiply_index(partials)\n",
    "            calculate_different_values(\n",
    "                partials, sum_histogram, exposure, a_norm, x_mean\n",
    "            )\n",
    "\n",
    "        return cuda.jit(exposure)\n",
    "\n",
    "    def _compile_4(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_4:\n",
    "            self._cache_4[key] = ExposureHistogramEqualization._gpu_kernel_factory_4(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_4[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_5(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "        max_block_size = (\n",
    "            ExposureHistogramEqualization._NUM_WARPS\n",
    "            * ExposureHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_left_hist(shared_memory, left_hist_size, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < left_hist_size:\n",
    "                shared_memory[local_tid] = arr[local_tid]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_right_hist(shared_memory, right_hist_size, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < right_hist_size:\n",
    "                shared_memory[local_tid] = arr[256 - right_hist_size + local_tid]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_sum_hist(hist_temp):\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(hist_temp, jump)\n",
    "                jump = jump * 2\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_pdf_left(d_cdf_clip_hist, left_hist_size, left_hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < left_hist_size:\n",
    "                temp = left_hist[local_tid]\n",
    "                left_hist[local_tid] = temp / d_cdf_clip_hist[left_hist_size - 1]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_pdf_right(d_cdf_clip_hist, right_hist_size, right_hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < right_hist_size:\n",
    "                temp = right_hist[local_tid]\n",
    "                right_hist[local_tid] = temp / (\n",
    "                    d_cdf_clip_hist[-1] - d_cdf_clip_hist[256 - right_hist_size - 1]\n",
    "                )\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf_hist(hist) -> None:\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(hist, jump)\n",
    "                jump *= 2\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def calculate_pdf_cdf(\n",
    "            d_clip_hist,\n",
    "            left_hist,\n",
    "            right_hist,\n",
    "            d_cdf_clip_hist,\n",
    "            left_hist_size,\n",
    "            right_hist_size,\n",
    "        ):\n",
    "            load_shared_memory_left_hist(left_hist, left_hist_size, d_clip_hist)\n",
    "            load_shared_memory_right_hist(right_hist, right_hist_size, d_clip_hist)\n",
    "            load_shared_memory_left_hist(d_cdf_clip_hist, 256, d_clip_hist)\n",
    "            calculate_sum_hist(d_cdf_clip_hist)\n",
    "            calculate_pdf_left(d_cdf_clip_hist, left_hist_size, left_hist)\n",
    "            calculate_pdf_right(d_cdf_clip_hist, right_hist_size, right_hist)\n",
    "            calculate_cdf_hist(left_hist)\n",
    "            calculate_cdf_hist(right_hist)\n",
    "\n",
    "        return cuda.jit(calculate_pdf_cdf)\n",
    "\n",
    "    def _compile_5(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_5:\n",
    "            self._cache_5[key] = ExposureHistogramEqualization._gpu_kernel_factory_5(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_5[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_6():\n",
    "        max_block_size = (\n",
    "            ExposureHistogramEqualization._NUM_WARPS\n",
    "            * ExposureHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_esihe(shared_memory, x_mean, left_hist, right_hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "\n",
    "            temp = shared_memory[local_tid]\n",
    "            if temp < x_mean + 1:\n",
    "                esihe_value = round(x_mean * left_hist[temp])\n",
    "            else:\n",
    "                esihe_value = round(\n",
    "                    (x_mean + 1) + (255 - x_mean) * right_hist[temp - x_mean - 1]\n",
    "                )\n",
    "\n",
    "            shared_memory[local_tid] = esihe_value\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def map_values(shared_memory, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            d_output[global_tid] = shared_memory[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def esihe(left_hist, right_hist, d_output, x_mean):\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.uint32\n",
    "            )\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            calculate_esihe(shared_memory_gray, x_mean, left_hist, right_hist)\n",
    "            map_values(shared_memory_gray, d_output)\n",
    "\n",
    "        return cuda.jit(esihe)\n",
    "\n",
    "    def _compile_6(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_6:\n",
    "            self._cache_6[key] = ExposureHistogramEqualization._gpu_kernel_factory_6()\n",
    "        return self._cache_6[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_7():\n",
    "        def reshape_1d_to_2d(d_input, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            width = d_output.shape[1]\n",
    "            if global_tid < d_input.size:\n",
    "                row = global_tid // width\n",
    "                column = global_tid % width\n",
    "\n",
    "                d_output[row, column] = d_input[global_tid]\n",
    "\n",
    "        return cuda.jit(reshape_1d_to_2d)\n",
    "\n",
    "    def _compile_7(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_7:\n",
    "            self._cache_7[key] = ExposureHistogramEqualization._gpu_kernel_factory_7()\n",
    "        return self._cache_7[key]\n",
    "\n",
    "    def __call__(self, d_input_orig):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        max_block_size = (\n",
    "            ExposureHistogramEqualization._WARP_SIZE\n",
    "            * ExposureHistogramEqualization._NUM_WARPS\n",
    "        )\n",
    "        stream = cuda.default_stream()\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width, 3), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_final_output_1d = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        d_final_output_2d = cuda.device_array(\n",
    "            shape=(height, width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "\n",
    "        d_hist = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        mean_histogram = cuda.device_array(shape=1, dtype=np.float32, stream=stream)\n",
    "        sum_histogram = cuda.device_array(shape=1, dtype=np.float32, stream=stream)\n",
    "        exposure = cuda.device_array(shape=1, dtype=np.float32, stream=stream)\n",
    "        a_norm = cuda.device_array(shape=1, dtype=np.float32, stream=stream)\n",
    "        x_mean = cuda.device_array(shape=1, dtype=np.uint8, stream=stream)\n",
    "        d_clip_hist = cuda.device_array(shape=256, dtype=np.float32, stream=stream)\n",
    "        d_cdf_clip_hist = cuda.device_array(shape=256, dtype=np.float32, stream=stream)\n",
    "\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(\n",
    "            d_input_orig.dtype,\n",
    "        )\n",
    "        kernel4 = self._compile_4(d_input_orig.dtype)\n",
    "        kernel5 = self._compile_5(d_input_orig.dtype)\n",
    "        kernel6 = self._compile_6(d_input_orig.dtype)\n",
    "        kernel7 = self._compile_7(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = min(max_block_size, d_input.size)\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "        kernel2[nb_blocks, nb_threads, stream](d_input, d_final_output_1d, d_hist)\n",
    "        kernel3[1, nb_threads, stream](\n",
    "            d_hist, d_clip_hist, mean_histogram, sum_histogram\n",
    "        )\n",
    "        kernel4[1, nb_threads, stream](\n",
    "            d_hist, sum_histogram[0], exposure, a_norm, x_mean\n",
    "        )\n",
    "        left_hist_size = x_mean[0] + 1\n",
    "        right_hist_size = 256 - left_hist_size\n",
    "        left_hist = cuda.device_array(\n",
    "            shape=(left_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        right_hist = cuda.device_array(\n",
    "            shape=(right_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "\n",
    "        kernel5[1, nb_threads, stream](\n",
    "            d_clip_hist,\n",
    "            left_hist,\n",
    "            right_hist,\n",
    "            d_cdf_clip_hist,\n",
    "            left_hist_size,\n",
    "            right_hist_size,\n",
    "        )\n",
    "\n",
    "        kernel6[nb_blocks, nb_threads, stream](\n",
    "            left_hist, right_hist, d_final_output_1d, x_mean[0]\n",
    "        )\n",
    "        kernel7[nb_blocks, nb_threads, stream](d_final_output_1d, d_final_output_2d)\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return (\n",
    "            cuda.event_elapsed_time(start_event, stop_event),\n",
    "            d_final_output_2d.copy_to_host(),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def esihe():\n",
    "        esihe = ExposureHistogramEqualization(lambda a, b: a + b)\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image = Image.open(image_bytes)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        d_input_image = cuda.to_device(image)\n",
    "        ct = []\n",
    "        for i in range(6):\n",
    "            if i == 0:\n",
    "                time, result_image = esihe(d_input_image)\n",
    "                result_image = Image.fromarray(result_image)\n",
    "                result_image.save(\"esihe.png\")\n",
    "            else:\n",
    "                time, _ = esihe(d_input_image)\n",
    "            ct.append(time)\n",
    "        print(f\"average kernel computation time is {np.average(ct[1:])} ms\")\n",
    "        print(ct)\n",
    "\n",
    "    esihe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMSICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average kernel computation time is 1.7328320026397706 ms\n",
      "[1328.797119140625, 1.8748480081558228, 1.6861439943313599, 1.7666239738464355, 1.7007360458374023, 1.635807991027832]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class MedianMeanHistogramEqualization(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "    _cache_4 = {}\n",
    "    _cache_5 = {}\n",
    "    _cache_6 = {}\n",
    "    _cache_7 = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_3d_to_2d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx][0] = d_input_orig[y, x][0]\n",
    "                d_input[idx][1] = d_input_orig[y, x][1]\n",
    "                d_input[idx][2] = d_input_orig[y, x][2]\n",
    "\n",
    "        return cuda.jit(reshape_3d_to_2d)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = MedianMeanHistogramEqualization._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = (\n",
    "            MedianMeanHistogramEqualization._NUM_WARPS\n",
    "            * MedianMeanHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_input\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_r[local_tid] = d_input[global_tid][0]\n",
    "            shared_memory_g[local_tid] = d_input[global_tid][1]\n",
    "            shared_memory_b[local_tid] = d_input[global_tid][2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_output\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            grayscale_value = round(\n",
    "                shared_memory_r[local_tid] * 0.299\n",
    "                + shared_memory_g[local_tid] * 0.587\n",
    "                + shared_memory_b[local_tid] * 0.114\n",
    "            )\n",
    "            d_output[global_tid] = grayscale_value\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory_histogram(shared_memory_histogram, d_hist):\n",
    "            tid = cuda.threadIdx.x\n",
    "            cuda.syncthreads()\n",
    "            while tid < max_block_size:\n",
    "                cuda.atomic.add(d_hist, tid, shared_memory_histogram[tid])\n",
    "                tid += cuda.blockDim.x\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory_histogram(shared_memory_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            shared_memory_histogram[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def histogram(d_input, d_output, d_hist):\n",
    "            shared_memory_red = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_green = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_blue = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "\n",
    "            load_shared_memory_input(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_input\n",
    "            )\n",
    "\n",
    "            grayscale_image(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_output\n",
    "            )\n",
    "\n",
    "            shared_memory_histogram = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            init_shared_memory_histogram(shared_memory_histogram)\n",
    "            tid = cuda.grid(1)\n",
    "            gridDim = cuda.gridDim.x * cuda.blockDim.x\n",
    "            while tid < d_output.size:\n",
    "                pixel_values = d_output[tid]\n",
    "                cuda.atomic.add(shared_memory_histogram, pixel_values, 1)\n",
    "                tid += gridDim\n",
    "            save_shared_memory_histogram(shared_memory_histogram, d_hist)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = MedianMeanHistogramEqualization._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "        max_block_size = (\n",
    "            MedianMeanHistogramEqualization._NUM_WARPS\n",
    "            * MedianMeanHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < arr.size:\n",
    "                shared_memory[local_tid] = arr[local_tid]\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < max_block_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf_histogram(d_hist, partials):\n",
    "            load_shared_memory(partials, d_hist)\n",
    "\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(partials, jump)\n",
    "                jump = jump * 2\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_median_histogram(partials, median_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            sum_histogram = partials[-1]\n",
    "            if (\n",
    "                local_tid > 0\n",
    "                and partials[local_tid] >= sum_histogram / 2\n",
    "                and partials[local_tid - 1] < sum_histogram / 2\n",
    "            ):\n",
    "                median_histogram[0] = local_tid\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def clip_histogram(d_hist, d_clip_hist, clip_value):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < max_block_size:\n",
    "                temp = d_hist[local_tid]\n",
    "                if temp > clip_value:\n",
    "                    d_clip_hist[local_tid] = clip_value\n",
    "                else:\n",
    "                    d_clip_hist[local_tid] = temp\n",
    "\n",
    "        def create_clip_histogram(d_hist, d_clip_hist, median_histogram):\n",
    "            partials = cuda.shared.array(shape=256, dtype=np.int32)\n",
    "            calculate_cdf_histogram(d_hist, partials)\n",
    "            calculate_median_histogram(partials, median_histogram)\n",
    "            clip_histogram(d_hist, d_clip_hist, d_hist[median_histogram[0]])\n",
    "\n",
    "        return cuda.jit(create_clip_histogram)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = MedianMeanHistogramEqualization._gpu_kernel_factory_3(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_4(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_left_hist(shared_memory, left_hist_size, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < left_hist_size:\n",
    "                shared_memory[local_tid] = arr[local_tid]\n",
    "            else:\n",
    "                shared_memory[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_right_hist(shared_memory, right_hist_size, arr):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < right_hist_size:\n",
    "                shared_memory[local_tid] = arr[256 - right_hist_size + local_tid]\n",
    "            else:\n",
    "                shared_memory[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump, hist_size):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < hist_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_sum_hist(hist_temp, hist_size):\n",
    "            jump = 1\n",
    "            while jump < hist_size:\n",
    "                pointer_jumping(hist_temp, jump, hist_size)\n",
    "                jump = jump * 2\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_pdf_hist(hist_temp, hist_size, hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < hist_size:\n",
    "                temp = hist[local_tid]\n",
    "                hist[local_tid] = temp / hist_temp[-1]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def multiply_left_index(hist, left_hist_size):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < left_hist_size:\n",
    "                temp = hist[local_tid]\n",
    "                hist[local_tid] = temp * local_tid\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def multiply_right_index(hist, right_hist_size):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < right_hist_size:\n",
    "                temp = hist[local_tid]\n",
    "                hist[local_tid] = temp * (local_tid + 256 - right_hist_size)\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def calculate_pdf_cdf(\n",
    "            d_hist,\n",
    "            left_hist,\n",
    "            right_hist,\n",
    "            left_hist_temp,\n",
    "            right_hist_temp,\n",
    "            left_hist_size,\n",
    "            right_hist_size,\n",
    "            x_ml,\n",
    "            x_mu,\n",
    "        ):\n",
    "            load_shared_memory_left_hist(left_hist, left_hist_size, d_hist)\n",
    "\n",
    "            load_shared_memory_right_hist(right_hist, right_hist_size, d_hist)\n",
    "            load_shared_memory_left_hist(left_hist_temp, left_hist_size, d_hist)\n",
    "            load_shared_memory_right_hist(right_hist_temp, right_hist_size, d_hist)\n",
    "            calculate_sum_hist(left_hist_temp, left_hist_size)\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            calculate_sum_hist(right_hist_temp, right_hist_size)\n",
    "\n",
    "            calculate_pdf_hist(left_hist_temp, left_hist_size, left_hist)\n",
    "            calculate_pdf_hist(right_hist_temp, right_hist_size, right_hist)\n",
    "            multiply_left_index(left_hist, left_hist_size)\n",
    "            multiply_right_index(right_hist, right_hist_size)\n",
    "\n",
    "            calculate_sum_hist(left_hist, left_hist_size)\n",
    "            calculate_sum_hist(right_hist, right_hist_size)\n",
    "\n",
    "            if cuda.threadIdx.x == 0:\n",
    "                x_ml[0] = round(left_hist[-1])\n",
    "                x_mu[0] = round(right_hist[-1])\n",
    "\n",
    "        return cuda.jit(calculate_pdf_cdf)\n",
    "\n",
    "    def _compile_4(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_4:\n",
    "            self._cache_4[key] = MedianMeanHistogramEqualization._gpu_kernel_factory_4(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_4[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_5(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_hist(\n",
    "            shared_memory_hist_clip,\n",
    "            d_clip_hist,\n",
    "            first_hist,\n",
    "            second_hist,\n",
    "            third_hist,\n",
    "            fourth_hist,\n",
    "            left_index,\n",
    "            median,\n",
    "            right_index,\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "\n",
    "            if local_tid <= left_index:\n",
    "                num_pixels = d_clip_hist[left_index]\n",
    "                first_hist[local_tid] = shared_memory_hist_clip[local_tid] / num_pixels\n",
    "            elif local_tid > left_index and local_tid <= median:\n",
    "                num_pixels = d_clip_hist[median] - d_clip_hist[left_index]\n",
    "                second_hist[local_tid - (left_index + 1)] = (\n",
    "                    shared_memory_hist_clip[local_tid] / num_pixels\n",
    "                )\n",
    "\n",
    "            elif local_tid > median and local_tid <= right_index:\n",
    "                num_pixels = d_clip_hist[right_index] - d_clip_hist[median]\n",
    "                third_hist[local_tid - (median + 1)] = (\n",
    "                    shared_memory_hist_clip[local_tid] / num_pixels\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                num_pixels = d_clip_hist[-1] - d_clip_hist[right_index]\n",
    "                fourth_hist[local_tid - (right_index + 1)] = (\n",
    "                    shared_memory_hist_clip[local_tid] / num_pixels\n",
    "                )\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(cdf, jump, hist_size):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            temp = cdf[tid]\n",
    "            cuda.syncthreads()\n",
    "            if right < hist_size:\n",
    "                cdf[right] = scan_op(temp, cdf[right])\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf_hist(hist, hist_size) -> None:\n",
    "            jump = 1\n",
    "            while jump < hist_size:\n",
    "                pointer_jumping(hist, jump, hist_size)\n",
    "                jump *= 2\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_hist(shared_memory_hist_clip, d_clip_hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "\n",
    "            shared_memory_hist_clip[local_tid] = d_clip_hist[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def calculate_pdf_cdf(\n",
    "            d_clip_hist,\n",
    "            first_hist,\n",
    "            second_hist,\n",
    "            third_hist,\n",
    "            fourth_hist,\n",
    "            x_ml,\n",
    "            x_mu,\n",
    "            median_value,\n",
    "        ):\n",
    "            shared_memory_hist_clip = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "            load_shared_memory_hist(shared_memory_hist_clip, d_clip_hist)\n",
    "            calculate_cdf_hist(d_clip_hist, 256)\n",
    "            load_hist(\n",
    "                shared_memory_hist_clip,\n",
    "                d_clip_hist,\n",
    "                first_hist,\n",
    "                second_hist,\n",
    "                third_hist,\n",
    "                fourth_hist,\n",
    "                x_ml,\n",
    "                median_value,\n",
    "                x_mu,\n",
    "            )\n",
    "\n",
    "            calculate_cdf_hist(first_hist, x_ml + 1)\n",
    "            calculate_cdf_hist(second_hist, median_value - x_ml)\n",
    "            calculate_cdf_hist(third_hist, x_mu - median_value)\n",
    "            calculate_cdf_hist(fourth_hist, 255 - x_mu)\n",
    "\n",
    "        return cuda.jit(calculate_pdf_cdf)\n",
    "\n",
    "    def _compile_5(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_5:\n",
    "            self._cache_5[key] = MedianMeanHistogramEqualization._gpu_kernel_factory_5(\n",
    "                self._functor\n",
    "            )\n",
    "        return self._cache_5[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_6():\n",
    "        max_block_size = (\n",
    "            MedianMeanHistogramEqualization._NUM_WARPS\n",
    "            * MedianMeanHistogramEqualization._WARP_SIZE\n",
    "        )\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_mmsiche(\n",
    "            shared_memory,\n",
    "            first_hist,\n",
    "            second_hist,\n",
    "            third_hist,\n",
    "            fourth_hist,\n",
    "            x_ml,\n",
    "            x_mu,\n",
    "            median_value,\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "\n",
    "            temp = shared_memory[local_tid]\n",
    "            if temp <= x_ml:\n",
    "                esihe_value = x_ml * first_hist[temp]\n",
    "            elif temp > x_ml and temp <= median_value:\n",
    "                esihe_value = (x_ml + 1) + second_hist[temp - x_ml - 1] * (\n",
    "                    median_value - x_ml + 1\n",
    "                )\n",
    "            elif temp > median_value and temp <= x_mu:\n",
    "                esihe_value = (median_value + 1) + third_hist[\n",
    "                    temp - median_value - 1\n",
    "                ] * (x_mu - median_value + 1)\n",
    "            elif temp > x_mu and temp < 256:\n",
    "                esihe_value = (x_mu + 1) + fourth_hist[temp - x_mu - 1] * (\n",
    "                    256 - x_mu + 1\n",
    "                )\n",
    "            shared_memory[local_tid] = esihe_value\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def map_values(shared_memory, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            d_output[global_tid] = shared_memory[local_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def mmsiche(\n",
    "            first_hist,\n",
    "            second_hist,\n",
    "            third_hist,\n",
    "            fourth_hist,\n",
    "            x_ml,\n",
    "            x_mu,\n",
    "            median_value,\n",
    "            d_output,\n",
    "        ):\n",
    "            shared_memory = cuda.shared.array(shape=max_block_size, dtype=np.uint32)\n",
    "            load_shared_memory(shared_memory, d_output)\n",
    "            calculate_mmsiche(\n",
    "                shared_memory,\n",
    "                first_hist,\n",
    "                second_hist,\n",
    "                third_hist,\n",
    "                fourth_hist,\n",
    "                x_ml,\n",
    "                x_mu,\n",
    "                median_value,\n",
    "            )\n",
    "            map_values(shared_memory, d_output)\n",
    "\n",
    "        return cuda.jit(mmsiche)\n",
    "\n",
    "    def _compile_6(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_6:\n",
    "            self._cache_6[key] = MedianMeanHistogramEqualization._gpu_kernel_factory_6()\n",
    "        return self._cache_6[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_7():\n",
    "        def reshape_1d_to_2d(d_input, d_output):\n",
    "            global_tid = cuda.grid(1)\n",
    "            width = d_output.shape[1]\n",
    "            if global_tid < d_input.size:\n",
    "                row = global_tid // width\n",
    "                column = global_tid % width\n",
    "\n",
    "                d_output[row, column] = d_input[global_tid]\n",
    "\n",
    "        return cuda.jit(reshape_1d_to_2d)\n",
    "\n",
    "    def _compile_7(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_7:\n",
    "            self._cache_7[key] = MedianMeanHistogramEqualization._gpu_kernel_factory_7()\n",
    "        return self._cache_7[key]\n",
    "\n",
    "    def __call__(self, d_input_orig):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        max_block_size = (\n",
    "            MedianMeanHistogramEqualization._WARP_SIZE\n",
    "            * MedianMeanHistogramEqualization._NUM_WARPS\n",
    "        )\n",
    "        stream = cuda.default_stream()\n",
    "\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width, 3), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_final_output_1d = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        d_final_output_2d = cuda.device_array(\n",
    "            shape=(height, width), dtype=np.uint8, stream=stream\n",
    "        )\n",
    "        median_histogram = cuda.device_array(shape=1, dtype=np.uint32, stream=stream)\n",
    "        d_hist = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        d_clip_hist = cuda.device_array(shape=256, dtype=np.float32, stream=stream)\n",
    "        x_ml = cuda.device_array(shape=1, dtype=np.uint32, stream=stream)\n",
    "        x_mu = cuda.device_array(shape=1, dtype=np.uint32, stream=stream)\n",
    "\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(\n",
    "            d_input_orig.dtype,\n",
    "        )\n",
    "        kernel4 = self._compile_4(d_input_orig.dtype)\n",
    "        kernel5 = self._compile_5(d_input_orig.dtype)\n",
    "        kernel6 = self._compile_6(d_input_orig.dtype)\n",
    "        kernel7 = self._compile_7(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "        nb_threads = max_block_size\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = min(max_block_size, d_input.size)\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "\n",
    "        kernel2[nb_blocks, nb_threads, stream](d_input, d_final_output_1d, d_hist)\n",
    "        kernel3[1, nb_threads, stream](d_hist, d_clip_hist, median_histogram)\n",
    "\n",
    "        left_hist_size = median_histogram[0]\n",
    "        right_hist_size = 256 - left_hist_size\n",
    "        left_hist = cuda.device_array(\n",
    "            shape=(left_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        right_hist = cuda.device_array(\n",
    "            shape=(right_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "\n",
    "        left_hist_temp = cuda.device_array(\n",
    "            shape=(left_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        right_hist_temp = cuda.device_array(\n",
    "            shape=(right_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        kernel4[1, nb_threads, stream](\n",
    "            d_hist,\n",
    "            left_hist,\n",
    "            right_hist,\n",
    "            left_hist_temp,\n",
    "            right_hist_temp,\n",
    "            left_hist_size,\n",
    "            right_hist_size,\n",
    "            x_ml,\n",
    "            x_mu,\n",
    "        )\n",
    "\n",
    "        first_hist_size = x_ml[0] + 1\n",
    "        first_hist = cuda.device_array(\n",
    "            shape=(first_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        second_hist_size = median_histogram[0] - x_ml[0]\n",
    "        second_hist = cuda.device_array(\n",
    "            shape=(second_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        third_hist_size = x_mu[0] - median_histogram[0]\n",
    "        third_hist = cuda.device_array(\n",
    "            shape=(third_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        fourth_hist_size = 255 - x_mu[0]\n",
    "        fourth_hist = cuda.device_array(\n",
    "            shape=(fourth_hist_size,), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        kernel5[1, nb_threads, stream](\n",
    "            d_clip_hist,\n",
    "            first_hist,\n",
    "            second_hist,\n",
    "            third_hist,\n",
    "            fourth_hist,\n",
    "            x_ml[0],\n",
    "            x_mu[0],\n",
    "            median_histogram[0],\n",
    "        )\n",
    "\n",
    "        kernel6[nb_blocks, nb_threads, stream](\n",
    "            first_hist,\n",
    "            second_hist,\n",
    "            third_hist,\n",
    "            fourth_hist,\n",
    "            x_ml[0],\n",
    "            x_mu[0],\n",
    "            median_histogram[0],\n",
    "            d_final_output_1d,\n",
    "        )\n",
    "        kernel7[nb_blocks, nb_threads, stream](d_final_output_1d, d_final_output_2d)\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return (\n",
    "            cuda.event_elapsed_time(start_event, stop_event),\n",
    "            d_final_output_2d.copy_to_host(),\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def mmsiche():\n",
    "        mmsiche = MedianMeanHistogramEqualization(lambda a, b: a + b)\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image = Image.open(image_bytes)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        d_input_image = cuda.to_device(image)\n",
    "\n",
    "        ct = []\n",
    "        for i in range(6):\n",
    "            if i == 0:\n",
    "                time, result_image = mmsiche(d_input_image)\n",
    "                result_image = Image.fromarray(result_image)\n",
    "                result_image.save(\"mmsiche.png\")\n",
    "\n",
    "            else:\n",
    "                time, _ = mmsiche(d_input_image)\n",
    "            ct.append(time)\n",
    "        print(f\"average kernel computation time is {np.average(ct[1:])} ms\")\n",
    "        print(ct)\n",
    "\n",
    "    mmsiche()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average kernel computation time is 2.1190207958221436 ms\n",
      "[993.296630859375, 2.1437759399414062, 2.1122241020202637, 2.110527992248535, 2.1091840267181396, 2.119391918182373]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import math\n",
    "\n",
    "class CLAHE(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(shared_memory_input, d_input):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            x, y = cuda.grid(2)\n",
    "            if y < d_input.shape[0] and x < d_input.shape[1]:\n",
    "                shared_memory_input[local_tid_y, local_tid_x, 0] = d_input[y, x, 0]\n",
    "                shared_memory_input[local_tid_y, local_tid_x, 1] = d_input[y, x, 1]\n",
    "                shared_memory_input[local_tid_y, local_tid_x, 2] = d_input[y, x, 2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            x, y = cuda.grid(2)\n",
    "            if y < d_output.shape[0] and x < d_output.shape[1]:\n",
    "                shared_memory_gray[local_tid_y, local_tid_x] = d_output[y, x]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(shared_memory_input, d_output):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            x, y = cuda.grid(2)\n",
    "            d_output[y, x] = round(\n",
    "                shared_memory_input[local_tid_y, local_tid_x, 0] * 0.299\n",
    "                + shared_memory_input[local_tid_y, local_tid_x, 1] * 0.587\n",
    "                + shared_memory_input[local_tid_y, local_tid_x, 2] * 0.114\n",
    "            )\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_histogram(shared_memory_gray, shared_memory_hist):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            temp = shared_memory_gray[local_tid_y, local_tid_x]\n",
    "            cuda.atomic.add(shared_memory_hist, temp, 1)\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_exceed(\n",
    "            shared_memory_hist, shared_memory_exceed, clip_limit\n",
    "        ):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            value = shared_memory_hist[local_tid_x * 16 + local_tid_y]\n",
    "            if value > clip_limit:\n",
    "                shared_memory_exceed[local_tid_x * 16 + local_tid_y] = (\n",
    "                    value - clip_limit\n",
    "                )\n",
    "                shared_memory_hist[local_tid_x * 16 + local_tid_y] = clip_limit\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_cdf(shared_memory_hist) -> None:\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            jump = 1\n",
    "            while jump < 256:\n",
    "                tid = local_tid_x * 16 + local_tid_y\n",
    "                right = tid + jump\n",
    "                temp = shared_memory_hist[tid]\n",
    "                cuda.syncthreads()\n",
    "                if right < 256:\n",
    "                    shared_memory_hist[right] = scan_op(temp, shared_memory_hist[right])\n",
    "                cuda.syncthreads()\n",
    "                jump *= 2\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def redistribute(shared_memory_hist, shared_memory_exceed, clip_limit):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            if local_tid_x == 0 and local_tid_y == 0:\n",
    "                total_exceed = shared_memory_exceed\n",
    "                bin_incr = total_exceed / 256\n",
    "                upper = clip_limit - bin_incr\n",
    "                for i in range(256):\n",
    "                    if shared_memory_hist[i] > clip_limit:\n",
    "                        shared_memory_hist[i] = clip_limit\n",
    "                    else:\n",
    "                        if shared_memory_hist[i] > upper:\n",
    "                            total_exceed += upper - shared_memory_hist[i]\n",
    "                            shared_memory_hist[i] = clip_limit\n",
    "                        else:\n",
    "                            total_exceed -= bin_incr\n",
    "                            shared_memory_hist[i] += bin_incr\n",
    "                    if total_exceed < 1:\n",
    "                        break\n",
    "\n",
    "                if total_exceed > 0:\n",
    "                    step_size = max(1, math.floor(1 + total_exceed / 256))\n",
    "                    for i in range(256):\n",
    "                        total_exceed -= step_size\n",
    "                        shared_memory_hist[i] += step_size\n",
    "                        if total_exceed < 1:\n",
    "                            break\n",
    "\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_cdf(cdf_all, shared_memory_hist):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            local_block_x = cuda.blockIdx.x\n",
    "            local_block_y = cuda.blockIdx.y\n",
    "            tid = local_tid_y * 16 + local_tid_x\n",
    "            cdf_all[local_block_y, local_block_x, tid] = shared_memory_hist[tid]\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory(shared_memory_hist):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            shared_memory_hist[local_tid_x * 16 + local_tid_y] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def normalized_cdf(shared_memory_hist, max_value):\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            tid = local_tid_x * 16 + local_tid_y\n",
    "            if max_value > 0 and tid < 256:\n",
    "                temp = shared_memory_hist[tid]\n",
    "                shared_memory_hist[tid] = (temp * 255) // max_value\n",
    "\n",
    "        def histogram(d_input, d_output, clip_limit, cdf_all):\n",
    "            shared_memory_input = cuda.shared.array(shape=(16, 16, 3), dtype=np.float32)\n",
    "            shared_memory_gray = cuda.shared.array(shape=(16, 16), dtype=np.float32)\n",
    "            shared_memory_hist = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "            shared_memory_exceed = cuda.shared.array(shape=256, dtype=np.float32)\n",
    "            load_shared_memory_input(shared_memory_input, d_input)\n",
    "            grayscale_image(shared_memory_input, d_output)\n",
    "            init_shared_memory(shared_memory_hist)\n",
    "            init_shared_memory(shared_memory_exceed)\n",
    "            load_shared_memory_gray(shared_memory_gray, d_output)\n",
    "            calculate_histogram(shared_memory_gray, shared_memory_hist)\n",
    "            load_shared_memory_exceed(\n",
    "                shared_memory_hist, shared_memory_exceed, clip_limit\n",
    "            )\n",
    "            calculate_cdf(shared_memory_exceed)\n",
    "            redistribute(shared_memory_hist, shared_memory_exceed[-1], clip_limit)\n",
    "            cuda.syncthreads()\n",
    "            calculate_cdf(shared_memory_hist)\n",
    "            normalized_cdf(shared_memory_hist, shared_memory_hist[-1])\n",
    "            cuda.syncthreads()\n",
    "            save_cdf(cdf_all, shared_memory_hist)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = CLAHE._gpu_kernel_factory(self._functor)\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        @cuda.jit(device=True)\n",
    "        def interpolate(d_output, cdf_all):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_output.shape[0]\n",
    "            width = d_output.shape[1]\n",
    "\n",
    "            x_dim = cuda.blockDim.x\n",
    "            y_dim = cuda.blockDim.y\n",
    "            local_tid_x = cuda.threadIdx.x\n",
    "            local_tid_y = cuda.threadIdx.y\n",
    "            num = x_dim * y_dim\n",
    "            if (\n",
    "                y >= y_dim / 2\n",
    "                and x >= x_dim / 2\n",
    "                and y < height - y_dim / 2\n",
    "                and x < width - x_dim / 2\n",
    "            ):\n",
    "                block_x = (x - 8) // x_dim + 1\n",
    "                block_y = (y - 8) // y_dim + 1\n",
    "                fourth_block_x, fourth_block_y = block_x, block_y\n",
    "                second_block_x, second_block_y = block_x - 1, block_y\n",
    "                third_block_x, third_block_y = block_x, block_y - 1\n",
    "                first_block_x, first_block_y = block_x - 1, block_y - 1\n",
    "\n",
    "                inverse_i = x_dim - ((x - 8) % x_dim)\n",
    "                inverse_j = y_dim - ((y - 8) % y_dim)\n",
    "                val = d_output[y, x]\n",
    "                if local_tid_x >= x_dim / 2 and local_tid_x < x_dim:\n",
    "                    j = local_tid_x - x_dim / 2\n",
    "                else:\n",
    "                    j = local_tid_x + x_dim / 2\n",
    "\n",
    "                if local_tid_y >= y_dim / 2 and local_tid_y < y_dim:\n",
    "                    i = local_tid_y - y_dim / 2\n",
    "                else:\n",
    "                    i = local_tid_y + y_dim / 2\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[first_block_y, first_block_x, val]\n",
    "                            + j * cdf_all[third_block_y, third_block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[second_block_y, second_block_x, val]\n",
    "                            + j * cdf_all[fourth_block_y, fourth_block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "            elif y < 8 and x < 8:\n",
    "                block_x = 0\n",
    "                block_y = 0\n",
    "                inverse_i = x_dim - x\n",
    "                inverse_j = y_dim - y\n",
    "                val = d_output[y, x]\n",
    "                j = local_tid_x\n",
    "                i = local_tid_y\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "            elif y >= height - y_dim / 2 and x >= width - x_dim / 2:\n",
    "                block_x = (x - 8) // x_dim\n",
    "                block_y = (y - 8) // y_dim\n",
    "                inverse_i = x_dim - (x % 8)\n",
    "                inverse_j = y_dim - (y % 8)\n",
    "                val = d_output[y, x]\n",
    "                j = local_tid_x - x_dim / 2\n",
    "                i = local_tid_y - y_dim / 2\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "            elif x < 8 and y >= height - y_dim / 2:\n",
    "                block_x = 0\n",
    "                block_y = (y - 8) // y_dim\n",
    "                inverse_i = x_dim - x\n",
    "                inverse_j = y_dim - (y % 8)\n",
    "                val = d_output[y, x]\n",
    "                j = local_tid_x\n",
    "                i = local_tid_y - y_dim / 2\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "            elif y < 8 and x >= width - x_dim / 2:\n",
    "                block_x = (x - 8) // x_dim\n",
    "                block_y = 0\n",
    "                inverse_i = x_dim - (x % 8)\n",
    "                inverse_j = y_dim - y\n",
    "                val = d_output[y, x]\n",
    "                i = local_tid_y\n",
    "                j = local_tid_x - x_dim / 2\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, block_x, val]\n",
    "                            + j * cdf_all[block_y, block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "            elif y >= 8 and y < height - y_dim / 2 and x < 8:\n",
    "                block_x = 0\n",
    "                first_block_y = (y - 8) // y_dim + 1\n",
    "                second_block_y = first_block_y - 1\n",
    "                inverse_i = x_dim - x\n",
    "                inverse_j = y_dim - ((y - 8) % y_dim)\n",
    "                val = d_output[y, x]\n",
    "                j = local_tid_x\n",
    "                if local_tid_y >= y_dim / 2 and local_tid_y < y_dim:\n",
    "                    i = local_tid_y - y_dim / 2\n",
    "                else:\n",
    "                    i = local_tid_y + y_dim / 2\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[first_block_y, block_x, val]\n",
    "                            + j * cdf_all[first_block_y, block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[second_block_y, block_x, val]\n",
    "                            + j * cdf_all[second_block_y, block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "\n",
    "            elif x >= 8 and x < width - x_dim / 2 and y < 8:\n",
    "                block_y = 0\n",
    "                first_block_x = (x - 8) // x_dim + 1\n",
    "                second_block_x = first_block_x - 1\n",
    "                inverse_i = x_dim - ((x - 8) % x_dim)\n",
    "                inverse_j = y_dim - y\n",
    "\n",
    "                val = d_output[y, x]\n",
    "                i = local_tid_y\n",
    "                if local_tid_x >= x_dim / 2 and local_tid_x < x_dim:\n",
    "                    j = local_tid_x - x_dim / 2\n",
    "                else:\n",
    "                    j = local_tid_x + x_dim / 2\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, first_block_x, val]\n",
    "                            + j * cdf_all[block_y, first_block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, second_block_x, val]\n",
    "                            + j * cdf_all[block_y, second_block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "\n",
    "            elif x >= 8 and x < width - x_dim / 2 and y >= height - y_dim / 2:\n",
    "                block_y = (y - 8) // y_dim\n",
    "                first_block_x = (x - 8) // x_dim + 1\n",
    "                second_block_x = first_block_x - 1\n",
    "                inverse_i = x_dim - ((x - 8) % x_dim)\n",
    "                inverse_j = y_dim - (y % 8)\n",
    "\n",
    "                val = d_output[y, x]\n",
    "                i = local_tid_y - y_dim / 2\n",
    "                if local_tid_x >= x_dim / 2 and local_tid_x < x_dim:\n",
    "                    j = local_tid_x - x_dim / 2\n",
    "                else:\n",
    "                    j = local_tid_x + x_dim / 2\n",
    "\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, first_block_x, val]\n",
    "                            + j * cdf_all[block_y, first_block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[block_y, second_block_x, val]\n",
    "                            + j * cdf_all[block_y, second_block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "\n",
    "            elif y >= 8 and y < height - y_dim / 2 and x >= width - x_dim / 2:\n",
    "                block_x = (x - 8) // x_dim\n",
    "                first_block_y = (y - 8) // y_dim + 1\n",
    "                second_block_y = first_block_y - 1\n",
    "                inverse_j = y_dim - ((y - 8) % y_dim)\n",
    "                inverse_i = x_dim - (x % 8)\n",
    "\n",
    "                val = d_output[y, x]\n",
    "                j = local_tid_x - x_dim / 2\n",
    "                if local_tid_y >= y_dim / 2 and local_tid_y < y_dim:\n",
    "                    i = local_tid_y - y_dim / 2\n",
    "                else:\n",
    "                    i = local_tid_y + y_dim / 2\n",
    "\n",
    "                d_output[y, x] = math.floor(\n",
    "                    (\n",
    "                        inverse_j\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[first_block_y, block_x, val]\n",
    "                            + j * cdf_all[first_block_y, block_x, val]\n",
    "                        )\n",
    "                        + i\n",
    "                        * (\n",
    "                            inverse_i * cdf_all[second_block_y, block_x, val]\n",
    "                            + j * cdf_all[second_block_y, block_x, val]\n",
    "                        )\n",
    "                    )\n",
    "                    / num\n",
    "                )\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def clahe(d_output, cdf_all):\n",
    "            interpolate(d_output, cdf_all)\n",
    "\n",
    "        return cuda.jit(clahe)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = CLAHE._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    def __call__(self, d_input_orig, clip_limit):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        stream = cuda.default_stream()\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (d_input_orig.shape[1] + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (d_input_orig.shape[0] + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        cdf_all = cuda.device_array((nb_blocks[1], nb_blocks[0], 256), dtype=np.float32)\n",
    "        d_output = cuda.device_array((height, width), dtype=np.uint8)\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        kernel[nb_blocks, nb_threads](d_input_orig, d_output, clip_limit, cdf_all)\n",
    "        kernel2[nb_blocks, nb_threads](d_output, cdf_all)\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return cuda.event_elapsed_time(start_event, stop_event), d_output.copy_to_host()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def clahe():\n",
    "        clahe = CLAHE(lambda a, b: a + b)\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image = Image.open(image_bytes)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        d_input_image = cuda.to_device(image)\n",
    "        clip_limit = 6\n",
    "        ct = []\n",
    "\n",
    "        for i in range(6):\n",
    "            if i == 0:\n",
    "                time, result_image = clahe(d_input_image, clip_limit)\n",
    "                result_image = Image.fromarray(result_image)\n",
    "                result_image.save(\"clahe.png\")\n",
    "            else:\n",
    "                time, _ = clahe(d_input_image, clip_limit)\n",
    "\n",
    "            ct.append(time)\n",
    "        print(f\"average kernel computation time is {np.average(ct[1:])} ms\")\n",
    "        print(ct)\n",
    "\n",
    "    clahe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path: he.png\n",
      "AME value:  55146700.0\n",
      "Image path: ahe.png\n",
      "AME value:  13814438.0\n",
      "Image path: whe.png\n",
      "AME value:  46403716.0\n",
      "Image path: esihe.png\n",
      "AME value:  20041760.0\n",
      "Image path: mmsiche.png\n",
      "AME value:  14356013.0\n",
      "Image path: clahe.png\n",
      "AME value:  35276290.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "class AMEReduce(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "    _cache_4 = {}\n",
    "    _cache_5 = {}\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor, functor2):\n",
    "        self._functor = functor\n",
    "        self._functor2 = functor2\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_3d_to_2d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx][0] = d_input_orig[y, x][0]\n",
    "                d_input[idx][1] = d_input_orig[y, x][1]\n",
    "                d_input[idx][2] = d_input_orig[y, x][2]\n",
    "\n",
    "        return cuda.jit(reshape_3d_to_2d)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = AMEReduce._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = AMEReduce._NUM_WARPS * AMEReduce._WARP_SIZE\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_input(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_input\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "\n",
    "            shared_memory_r[local_tid] = d_input[global_tid][0]\n",
    "            shared_memory_g[local_tid] = d_input[global_tid][1]\n",
    "            shared_memory_b[local_tid] = d_input[global_tid][2]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def grayscale_image(\n",
    "            shared_memory_r, shared_memory_g, shared_memory_b, d_output\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            grayscale_value = round(\n",
    "                shared_memory_r[local_tid] * 0.299\n",
    "                + shared_memory_g[local_tid] * 0.587\n",
    "                + shared_memory_b[local_tid] * 0.114\n",
    "            )\n",
    "            d_output[global_tid] = grayscale_value\n",
    "\n",
    "        def grayscale(d_input, d_output):\n",
    "            shared_memory_red = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_green = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_blue = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            load_shared_memory_input(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_input\n",
    "            )\n",
    "\n",
    "            grayscale_image(\n",
    "                shared_memory_red, shared_memory_green, shared_memory_blue, d_output\n",
    "            )\n",
    "\n",
    "        return cuda.jit(grayscale)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = AMEReduce._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3():\n",
    "        def reshape_2d_to_1d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx] = d_input_orig[y, x]\n",
    "\n",
    "        return cuda.jit(reshape_2d_to_1d)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = AMEReduce._gpu_kernel_factory_3()\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_4(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "        max_block_size = AMEReduce._NUM_WARPS * AMEReduce._WARP_SIZE\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(\n",
    "            shared_memory_image_1,\n",
    "            d_image_1,\n",
    "            shared_memory_image_2,\n",
    "            d_image_2,\n",
    "        ):\n",
    "            global_tid = cuda.grid(1)\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if global_tid < d_image_1.size:\n",
    "                shared_memory_image_1[local_tid] = d_image_1[global_tid]\n",
    "                shared_memory_image_2[local_tid] = d_image_2[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pixel_difference(\n",
    "            shared_memory_image_1, shared_memory_image_2, d_image_diff\n",
    "        ):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            if global_tid < d_image_diff.size:\n",
    "                temp = (\n",
    "                    scan_op(\n",
    "                        shared_memory_image_1[local_tid],\n",
    "                        shared_memory_image_2[local_tid],\n",
    "                    )\n",
    "                    if shared_memory_image_1[local_tid]\n",
    "                    > shared_memory_image_2[local_tid]\n",
    "                    else scan_op(\n",
    "                        shared_memory_image_2[local_tid],\n",
    "                        shared_memory_image_1[local_tid],\n",
    "                    )\n",
    "                )\n",
    "                d_image_diff[global_tid] = temp\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def calculate_pixel_diff(\n",
    "            d_image_1,\n",
    "            d_image_2,\n",
    "            d_image_diff,\n",
    "        ):\n",
    "            shared_memory_image_1 = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            shared_memory_image_2 = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.float32\n",
    "            )\n",
    "            load_shared_memory(\n",
    "                shared_memory_image_1,\n",
    "                d_image_1,\n",
    "                shared_memory_image_2,\n",
    "                d_image_2,\n",
    "            )\n",
    "            pixel_difference(shared_memory_image_1, shared_memory_image_2, d_image_diff)\n",
    "\n",
    "        return cuda.jit(calculate_pixel_diff)\n",
    "\n",
    "    def _compile_4(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_4:\n",
    "            self._cache_4[key] = AMEReduce._gpu_kernel_factory_4(self._functor)\n",
    "        return self._cache_4[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_5(fn):\n",
    "        reduce_op = cuda.jit(device=True)(fn)\n",
    "        max_block_size = AMEReduce._NUM_WARPS * AMEReduce._WARP_SIZE\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory(shared_memory, arr, null_value):\n",
    "            global_tid = cuda.grid(1)\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            number_of_all_threads = cuda.gridsize(1)\n",
    "            sum = null_value\n",
    "            while global_tid < arr.size:\n",
    "                sum = reduce_op(sum, arr[global_tid])\n",
    "                global_tid += number_of_all_threads\n",
    "            shared_memory[local_tid] = sum\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def reduce_per_warp(shared_memory):\n",
    "            in_warp_id = cuda.threadIdx.x % 32\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            jump = 1\n",
    "            while jump < 32:\n",
    "                if jump + in_warp_id < 32:\n",
    "                    temp = shared_memory[tid + jump]\n",
    "                cuda.syncwarp()\n",
    "                if jump + in_warp_id < 32:\n",
    "                    shared_memory[tid] = reduce_op(shared_memory[tid], temp)\n",
    "                cuda.syncwarp()\n",
    "                jump = jump * 2\n",
    "\n",
    "        def gpu_reduce_block(arr, partials, null_value):\n",
    "            shared_memory = cuda.shared.array(shape=max_block_size, dtype=np.float32)\n",
    "            load_shared_memory(shared_memory, arr, null_value)\n",
    "            tid = cuda.threadIdx.x\n",
    "            reduce_per_warp(shared_memory)\n",
    "            cuda.syncthreads()\n",
    "            if tid >= 32:\n",
    "                return\n",
    "            to_warp = tid * 32\n",
    "            if to_warp < cuda.blockDim.x:\n",
    "                shared_memory[tid] = shared_memory[to_warp]\n",
    "            else:\n",
    "                shared_memory[tid] = null_value\n",
    "            reduce_per_warp(shared_memory)\n",
    "\n",
    "            if cuda.threadIdx.x == 0:\n",
    "                blk_id = cuda.blockIdx.x\n",
    "                partials[blk_id] = shared_memory[0]\n",
    "\n",
    "        return cuda.jit(gpu_reduce_block)\n",
    "\n",
    "    def _compile_5(self, dtype):\n",
    "        key = self._functor2, dtype\n",
    "        if key not in self._cache_5:\n",
    "            self._cache_5[key] = AMEReduce._gpu_kernel_factory_5(self._functor2)\n",
    "        return self._cache_5[key]\n",
    "\n",
    "    def __call__(self, d_input_orig, d_input_he_orig, stream=cuda.default_stream()):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width, 3), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_input_gray = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_input_he = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.float32, stream=stream\n",
    "        )\n",
    "        d_image_diff = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.float32, stream=stream\n",
    "        )\n",
    "\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(d_input_orig.dtype)\n",
    "        kernel4 = self._compile_4(d_input_orig.dtype)\n",
    "        kernel5 = self._compile_5(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        # Reshape the original image\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = 256\n",
    "        nb_blocks = (d_input.shape[0] + nb_threads - 1) // nb_threads\n",
    "\n",
    "        # Convert the original image to grayscale\n",
    "        kernel2[nb_blocks, nb_threads, stream](d_input, d_input_gray)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "\n",
    "        # Flatten the resulting image\n",
    "        kernel3[nb_blocks, nb_threads, stream](d_input_he_orig, d_input_he)\n",
    "\n",
    "        nb_threads = 256\n",
    "        nb_blocks = (d_image_diff.size + nb_threads - 1) // nb_threads\n",
    "        kernel4[nb_blocks, nb_threads, stream](d_input_gray, d_input_he, d_image_diff)\n",
    "        null_value = np.float32(0)\n",
    "        while True:\n",
    "            if nb_threads > d_image_diff.size:\n",
    "                nb_threads = d_image_diff.size\n",
    "            nb_blocks = min(\n",
    "                nb_threads, (d_image_diff.size + nb_threads - 1) // nb_threads\n",
    "            )\n",
    "\n",
    "            temp = cuda.device_array(\n",
    "                shape=nb_blocks, dtype=d_image_diff.dtype, stream=stream\n",
    "            )\n",
    "\n",
    "            kernel5[nb_blocks, nb_threads, stream](d_image_diff, temp, null_value)\n",
    "            cuda.synchronize()\n",
    "            d_image_diff = temp\n",
    "            if d_image_diff.size == 1:\n",
    "                break\n",
    "\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return d_image_diff.copy_to_host(stream=stream)[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def calculate_ame(image_path):\n",
    "        image_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/000cf025-48d6-45b2-987f-c19db5250699/width=1024/image%20(18).jpeg\"\n",
    "        response = requests.get(image_url)\n",
    "        image_bytes = BytesIO(response.content)\n",
    "        image_original = Image.open(image_bytes)\n",
    "        image_original = np.array(image_original, dtype=np.float32)\n",
    "        image_he = Image.open(image_path)\n",
    "        image_he = np.array(image_he, dtype=np.float32)\n",
    "        ame = AMEReduce(lambda a, b: a - b, lambda a, b: a + b)\n",
    "        d_image = cuda.to_device(image_original)\n",
    "        d_image_he = cuda.to_device(image_he)\n",
    "        ame_value = ame(d_image, d_image_he)\n",
    "        print(\"AME value: \", ame_value)\n",
    "\n",
    "    image_paths = [\n",
    "        \"he.png\",\n",
    "        \"ahe.png\",\n",
    "        \"whe.png\",\n",
    "        \"esihe.png\",\n",
    "        \"mmsiche.png\",\n",
    "        \"clahe.png\",\n",
    "    ]\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Image path: {image_path}\")\n",
    "        calculate_ame(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path: he.png\n",
      "Entropy value:  7.3262386\n",
      "Image path: ahe.png\n",
      "Entropy value:  7.4778366\n",
      "Image path: whe.png\n",
      "Entropy value:  7.3425884\n",
      "Image path: esihe.png\n",
      "Entropy value:  7.476408\n",
      "Image path: mmsiche.png\n",
      "Entropy value:  7.424331\n",
      "Image path: clahe.png\n",
      "Entropy value:  7.602747\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "class Entropy(object):\n",
    "    _cache = {}\n",
    "    _cache_2 = {}\n",
    "    _cache_3 = {}\n",
    "\n",
    "    _WARP_SIZE = 32\n",
    "    _NUM_WARPS = 8\n",
    "\n",
    "    def __init__(self, functor):\n",
    "        self._functor = functor\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory():\n",
    "        def reshape_2d_to_1d(d_input_orig, d_input):\n",
    "            x, y = cuda.grid(2)\n",
    "            height = d_input_orig.shape[0]\n",
    "            width = d_input_orig.shape[1]\n",
    "            if x < width and y < height:\n",
    "                idx = y * width + x\n",
    "                d_input[idx] = d_input_orig[y, x]\n",
    "\n",
    "        return cuda.jit(reshape_2d_to_1d)\n",
    "\n",
    "    def _compile(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache:\n",
    "            self._cache[key] = Entropy._gpu_kernel_factory()\n",
    "        return self._cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2():\n",
    "        max_block_size = Entropy._NUM_WARPS * Entropy._WARP_SIZE\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def load_shared_memory_gray(shared_memory_gray, d_output):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            global_tid = cuda.grid(1)\n",
    "            shared_memory_gray[local_tid] = d_output[global_tid]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def save_shared_memory_histogram(shared_memory_histogram, d_hist):\n",
    "            tid = cuda.threadIdx.x\n",
    "            cuda.syncthreads()\n",
    "            while tid < max_block_size:\n",
    "                cuda.atomic.add(d_hist, tid, shared_memory_histogram[tid])\n",
    "                tid += cuda.blockDim.x\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def init_shared_memory_histogram(shared_memory_histogram):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            shared_memory_histogram[local_tid] = 0\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def histogram(d_input, d_hist):\n",
    "            shared_memory_gray = cuda.shared.array(\n",
    "                shape=max_block_size, dtype=np.uint32\n",
    "            )\n",
    "            shared_memory_histogram = cuda.shared.array(shape=256, dtype=np.uint32)\n",
    "            load_shared_memory_gray(shared_memory_gray, d_input)\n",
    "            init_shared_memory_histogram(shared_memory_histogram)\n",
    "            tid = cuda.grid(1)\n",
    "            gridDim = cuda.gridDim.x * cuda.blockDim.x\n",
    "            while tid < d_input.size:\n",
    "                pixel_values = d_input[tid]\n",
    "                cuda.atomic.add(shared_memory_histogram, pixel_values, 1)\n",
    "                tid += gridDim\n",
    "            save_shared_memory_histogram(shared_memory_histogram, d_hist)\n",
    "\n",
    "        return cuda.jit(histogram)\n",
    "\n",
    "    def _compile_2(self, dtype):\n",
    "        key = dtype\n",
    "        if key not in self._cache_2:\n",
    "            self._cache_2[key] = Entropy._gpu_kernel_factory_2()\n",
    "        return self._cache_2[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_3(fn):\n",
    "        scan_op = cuda.jit(device=True)(fn)\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_normalized_histogram(d_image, d_hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            num_pixels = d_image.size\n",
    "            if local_tid < cuda.blockDim.x:\n",
    "                temp = d_hist[local_tid]\n",
    "                d_hist[local_tid] = temp / num_pixels\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def calculate_entropy(d_hist):\n",
    "            local_tid = cuda.threadIdx.x\n",
    "            if local_tid < cuda.blockDim.x:\n",
    "                temp = d_hist[local_tid]\n",
    "                if temp == 0:\n",
    "                    entropy = 0\n",
    "                else:\n",
    "                    entropy = -temp * math.log2(temp)\n",
    "                d_hist[local_tid] = entropy\n",
    "\n",
    "        @cuda.jit(device=True)\n",
    "        def pointer_jumping(d_hist, jump):\n",
    "            tid = cuda.threadIdx.x\n",
    "\n",
    "            right = tid + jump\n",
    "            if right < cuda.blockDim.x:\n",
    "                temp = d_hist[right]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            if right < cuda.blockDim.x:\n",
    "                d_hist[right] = scan_op(d_hist[tid], temp)\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        def entropy(d_image, d_hist):\n",
    "            calculate_normalized_histogram(d_image, d_hist)\n",
    "            calculate_entropy(d_hist)\n",
    "\n",
    "            jump = 1\n",
    "            while jump < cuda.blockDim.x:\n",
    "                pointer_jumping(d_hist, jump)\n",
    "                jump *= 2\n",
    "\n",
    "        return cuda.jit(entropy)\n",
    "\n",
    "    def _compile_3(self, dtype):\n",
    "        key = self._functor, dtype\n",
    "        if key not in self._cache_3:\n",
    "            self._cache_3[key] = Entropy._gpu_kernel_factory_3(self._functor)\n",
    "        return self._cache_3[key]\n",
    "\n",
    "    def __call__(self, d_input_orig, stream=cuda.default_stream()):\n",
    "        height, width = d_input_orig.shape[0], d_input_orig.shape[1]\n",
    "        _sav, core.config.CUDA_LOW_OCCUPANCY_WARNINGS = (\n",
    "            core.config.CUDA_LOW_OCCUPANCY_WARNINGS,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        d_input = cuda.device_array(\n",
    "            shape=(height * width), dtype=np.float32, stream=stream\n",
    "        )\n",
    "\n",
    "        d_hist = cuda.device_array(shape=(256,), dtype=np.float32, stream=stream)\n",
    "        kernel = self._compile(d_input_orig.dtype)\n",
    "        kernel2 = self._compile_2(d_input_orig.dtype)\n",
    "        kernel3 = self._compile_3(d_input_orig.dtype)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        nb_threads = (16, 16)\n",
    "        nb_blocks = (\n",
    "            (width + nb_threads[1] - 1) // nb_threads[1],\n",
    "            (height + nb_threads[0] - 1) // nb_threads[0],\n",
    "        )\n",
    "        kernel[nb_blocks, nb_threads, stream](d_input_orig, d_input)\n",
    "\n",
    "        nb_threads = self._NUM_WARPS * self._WARP_SIZE\n",
    "        nb_blocks = (d_input.size + nb_threads - 1) // nb_threads\n",
    "        kernel2[nb_blocks, nb_threads, stream](d_input, d_hist)\n",
    "        kernel3[1, nb_threads, stream](d_input, d_hist)\n",
    "        stop_event = cuda.event(True)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        core.config.CUDA_LOW_OCCUPANCY_WARNINGS = _sav\n",
    "\n",
    "        return d_hist.copy_to_host(stream=stream)[-1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def calculate_entropy(image_path):\n",
    "        image_he = Image.open(image_path)\n",
    "        image_he = np.array(image_he, dtype=np.float32)\n",
    "        en = Entropy(lambda a, b: a + b)\n",
    "        d_input_image = cuda.to_device(image_he)\n",
    "        entropy_value = en(d_input_image)\n",
    "        print(\"Entropy value: \", entropy_value)\n",
    "        \n",
    "    image_paths = [\n",
    "        \"he.png\",\n",
    "        \"ahe.png\",\n",
    "        \"whe.png\",\n",
    "        \"esihe.png\",\n",
    "        \"mmsiche.png\",\n",
    "        \"clahe.png\",\n",
    "    ]\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Image path: {image_path}\")\n",
    "        calculate_entropy(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
